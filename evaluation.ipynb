{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jit9XdFMRD9k"
      },
      "source": [
        "# **Cultural Bias Classification - Evaluation - Yelizaveta Tskhe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0pTJfnMsGf5"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkdCspEo2uj",
        "outputId": "358616fb-a087-48df-adeb-b34ab773524a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.4)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install SPARQLWrapper\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install transformers[torch]\n",
        "!pip install safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeFKG0zQrzHV"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed\n",
        ")\n",
        "import evaluate\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFBjKTtvprgL"
      },
      "source": [
        "## NON-LM-BASED APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX0IuPlrnpdE",
        "outputId": "eb67ca1b-0566-4409-8069-25debb5dc125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved as no_bias_test_output_nonlm.csv.\n"
          ]
        }
      ],
      "source": [
        "CACHE_FILE = \"wikidata_cache.json\"\n",
        "\n",
        "if os.path.exists(CACHE_FILE):\n",
        "    with open(CACHE_FILE, 'r') as f:\n",
        "        wikidata_cache = json.load(f)\n",
        "else:\n",
        "    wikidata_cache = {}\n",
        "\n",
        "path_testset = 'test_unlabeled.csv'\n",
        "test_df = pd.read_csv(path_testset)\n",
        "\n",
        "def get_wikidata_category_info(wikidata_id, max_retries=3):\n",
        "    if wikidata_id in wikidata_cache:\n",
        "        return wikidata_cache[wikidata_id][\"values\"], wikidata_cache[wikidata_id][\"property_values\"]\n",
        "\n",
        "    wikidata_id_clean = wikidata_id.split(\"/\")[-1]  # clean ID\n",
        "\n",
        "    try:\n",
        "        sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "        sparql.addCustomHttpHeader(\"User-Agent\", \"CulturalClassifier/1.0 (research-project)\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT ?propertyLabel ?valueLabel WHERE {{\n",
        "          VALUES ?prop {{\n",
        "            wdt:P31  # instance of\n",
        "            wdt:P279  # subclass of\n",
        "            wdt:P495  # country of origin\n",
        "            wdt:P27  # country of citizenship\n",
        "            wdt:P361  # part of\n",
        "            wdt:P1535  # used by\n",
        "            wdt:P131  # located in administrative territory\n",
        "            wdt:P17  # country\n",
        "            wdt:P276  # location\n",
        "            wdt:P3342  # significant place\n",
        "          }}\n",
        "          wd:{wikidata_id_clean} ?prop ?value .\n",
        "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language 'en'. }}\n",
        "        }}\n",
        "        LIMIT 20\n",
        "        \"\"\"\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                sparql.setQuery(query)\n",
        "                sparql.setReturnFormat(JSON)\n",
        "                results = sparql.query().convert()\n",
        "\n",
        "                values = [result[\"valueLabel\"][\"value\"].lower() for result in results[\"results\"][\"bindings\"]]\n",
        "                property_values = []\n",
        "                for result in results[\"results\"][\"bindings\"]:\n",
        "                    property_values.append({\n",
        "                        \"property\": result[\"propertyLabel\"][\"value\"].lower() if \"propertyLabel\" in result else \"\",\n",
        "                        \"value\": result[\"valueLabel\"][\"value\"].lower() if \"valueLabel\" in result else \"\"\n",
        "                    })\n",
        "\n",
        "                # cache the results\n",
        "                wikidata_cache[wikidata_id] = {\n",
        "                    \"values\": values,\n",
        "                    \"property_values\": property_values\n",
        "                }\n",
        "\n",
        "                # save to cache file occasionally\n",
        "                if random.random() < 0.01:\n",
        "                    with open(CACHE_FILE, 'w') as f:\n",
        "                        json.dump(wikidata_cache, f)\n",
        "\n",
        "                return values, property_values\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"Rate limit hit for {wikidata_id_clean}, waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"SPARQL error for {wikidata_id_clean}: {e}\")\n",
        "                    break\n",
        "\n",
        "        print(f\"All SPARQL attempts failed for {wikidata_id_clean}\")\n",
        "        return [], []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SPARQL setup error for {wikidata_id_clean}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def extract_features_with_wikidata(sample):\n",
        "    is_entity = 1 if sample['type'] == 'entity' else 0\n",
        "    is_concept = 1 if sample['type'] == 'concept' else 0\n",
        "    desc_length = len(sample['description'])\n",
        "\n",
        "    categories = ['politics', 'food', 'fashion', 'sports', 'music', 'films', 'literature']\n",
        "    category_features = [1 if sample['category'] == cat else 0 for cat in categories]\n",
        "\n",
        "    description = sample['description'].lower()\n",
        "    common_countries = [\"italy\", \"france\", \"japan\", \"china\", \"india\", \"germany\", \"spain\", \"brazil\", \"mexico\",\n",
        "                       \"american\", \"european\", \"african\", \"asian\", \"australia\", \"canada\", \"russia\"]\n",
        "    has_country = int(any(country in description for country in common_countries))\n",
        "\n",
        "    global_keywords = [\"global\", \"universal\", \"worldwide\", \"international\",\n",
        "                       \"common\", \"general\", \"standard\", \"widely used\", \"across cultures\",\n",
        "                       \"ubiquitous\", \"everywhere\", \"throughout the world\", \"many countries\"]\n",
        "    has_global_term = int(any(term in description for term in global_keywords))\n",
        "\n",
        "    country_count = sum(country in description for country in common_countries)\n",
        "\n",
        "    try:\n",
        "        wikidata_values, property_values = get_wikidata_category_info(sample['item'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Wikidata query: {e}\")\n",
        "        wikidata_values, property_values = [], []\n",
        "\n",
        "    # unique countries in Wikidata\n",
        "    country_entities = set()\n",
        "    for value in wikidata_values:\n",
        "        for country in common_countries:\n",
        "            if country in value.lower():\n",
        "                country_entities.add(country)\n",
        "    wikidata_country_count = len(country_entities)\n",
        "\n",
        "    cultural_keywords = [\"culture\", \"heritage\", \"tradition\", \"ethnic\", \"korean\", \"japanese\", \"french\",\n",
        "                        \"italian\", \"german\", \"american\", \"african\", \"traditional\", \"indigenous\"]\n",
        "    has_cultural_label = int(any(kw in value for value in wikidata_values for kw in cultural_keywords))\n",
        "\n",
        "    # for agnostic items\n",
        "    absence_of_cultural_markers = 1 if not has_cultural_label and not has_country else 0\n",
        "\n",
        "    # \"used by\" property with multiple values\n",
        "    used_by_count = sum(1 for pv in property_values if \"used by\" in pv[\"property\"])\n",
        "\n",
        "    # global categories that tend to be culturally agnostic\n",
        "    global_categories = [\"tool\", \"equipment\", \"device\", \"instrument\", \"technology\", \"scientific\", \"mathematical\"]\n",
        "    is_global_category = int(any(gc in value for value in wikidata_values for gc in global_categories))\n",
        "\n",
        "    # items with global usage but specific origin\n",
        "    has_origin = int(any(\"origin\" in pv[\"property\"] for pv in property_values))\n",
        "    global_but_specific_origin = 1 if has_global_term and has_origin else 0\n",
        "\n",
        "    cr_markers = [\"widespread\", \"popular\", \"common\", \"adopted\", \"adapted\",\n",
        "                 \"international\", \"regional\", \"variant\", \"inspired\", \"influence\"]\n",
        "    cr_marker_count = sum(term in description for term in cr_markers)\n",
        "\n",
        "    ce_markers = [\"unique\", \"exclusive\", \"specific\", \"only\", \"indigenous\",\n",
        "                 \"traditional\", \"native\", \"authentic\", \"original\", \"exclusive to\"]\n",
        "    ce_marker_count = sum(term in description for term in ce_markers)\n",
        "\n",
        "    adoption_exclusivity_ratio = cr_marker_count / max(ce_marker_count, 1)\n",
        "\n",
        "    # ce (more specific locations/cultures)\n",
        "    named_entities = len(re.findall(r'[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*', sample['description']))\n",
        "\n",
        "    return [is_entity, is_concept, desc_length, has_country, has_cultural_label,\n",
        "            has_global_term, country_count, wikidata_country_count, absence_of_cultural_markers,\n",
        "            used_by_count, is_global_category, global_but_specific_origin,\n",
        "            cr_marker_count, ce_marker_count, adoption_exclusivity_ratio, named_entities] + category_features\n",
        "\n",
        "\n",
        "def predict_two_stage(X, model_stage1, model_stage2):\n",
        "    stage1_preds = model_stage1.predict(X)\n",
        "\n",
        "    final_preds = ['cultural agnostic' if pred == 'CA' else None for pred in stage1_preds]\n",
        "\n",
        "    non_ca_indices = [i for i, pred in enumerate(stage1_preds) if pred == 'non-CA']\n",
        "\n",
        "    if len(non_ca_indices) > 0:\n",
        "        X_non_ca = X[non_ca_indices]\n",
        "        stage2_preds = model_stage2.predict(X_non_ca)\n",
        "\n",
        "        for idx, pred in zip(non_ca_indices, stage2_preds):\n",
        "            final_preds[idx] = pred\n",
        "\n",
        "    return final_preds\n",
        "\n",
        "def post_process_cr_ce_predictions(predictions, X_val, feature_names):\n",
        "    adjusted_predictions = predictions.copy()\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "        # convert features array to dictionary for easier access\n",
        "        features = dict(zip(feature_names, X_val[i]))\n",
        "\n",
        "        # rule 1\n",
        "        if features.get('has_global_term', 0) == 1 and features.get('has_cultural_label', 0) == 0:\n",
        "            adjusted_predictions[i] = 'cultural agnostic'\n",
        "\n",
        "        # rule 2: many countries mentioned -> CA\n",
        "        if features.get('country_count', 0) >= 3 or features.get('wikidata_country_count', 0) >= 3:\n",
        "            adjusted_predictions[i] = 'cultural agnostic'\n",
        "\n",
        "        # rule 3: global category items -> CA\n",
        "        if features.get('is_global_category', 0) == 1 and features.get('has_cultural_label', 0) == 0:\n",
        "            adjusted_predictions[i] = 'cultural agnostic'\n",
        "\n",
        "        if adjusted_predictions[i] != 'cultural agnostic':\n",
        "            # rule 1: high adoption ratio -> ce\n",
        "            if features.get('adoption_exclusivity_ratio', 0) > 2.0:\n",
        "                adjusted_predictions[i] = 'cultural representative'\n",
        "\n",
        "            # rule 2: high exclusivity marker -> ce\n",
        "            if features.get('ce_marker_count', 0) > 3 and features.get('cr_marker_count', 0) < 2:\n",
        "                adjusted_predictions[i] = 'cultural exclusive'\n",
        "\n",
        "            # rule 3: high named entity -> ce\n",
        "            if features.get('named_entities', 0) > 3:\n",
        "                adjusted_predictions[i] = 'cultural exclusive'\n",
        "\n",
        "    return adjusted_predictions\n",
        "\n",
        "def extract_features_for_test_set(test_df):\n",
        "    X_test = []\n",
        "    for _, row in test_df.iterrows():\n",
        "        sample = row.to_dict()\n",
        "        features = extract_features_with_wikidata(sample)\n",
        "        X_test.append(features)\n",
        "    return np.array(X_test)\n",
        "\n",
        "X_test = extract_features_for_test_set(test_df)\n",
        "\n",
        "model_stage1 = joblib.load('model_stage1_nonlm.pkl')\n",
        "model_stage2 = joblib.load('model_stage2_nonlm.pkl')\n",
        "\n",
        "y_pred_test = predict_two_stage(X_test, model_stage1, model_stage2)\n",
        "feature_names = ['is_entity', 'is_concept', 'desc_length', 'has_country', 'has_cultural_label',\n",
        "                     'has_global_term', 'country_count', 'wikidata_country_count', 'absence_of_cultural_markers',\n",
        "                     'used_by_count', 'is_global_category', 'global_but_specific_origin',\n",
        "                     'cr_marker_count', 'ce_marker_count', 'adoption_exclusivity_ratio', 'named_entities',\n",
        "                     'category_politics', 'category_food', 'category_fashion',\n",
        "                     'category_sports', 'category_music', 'category_films',\n",
        "                     'category_literature']\n",
        "\n",
        "y_pred_test = post_process_cr_ce_predictions(y_pred_test, X_test, feature_names)\n",
        "\n",
        "output_data = []\n",
        "for i, sample in test_df.iterrows():\n",
        "    output_data.append({\n",
        "        'item': sample['item'],\n",
        "        'name': sample['name'],\n",
        "        'description': sample['description'],\n",
        "        'type': sample['type'],\n",
        "        'category': sample['category'],\n",
        "        'label': y_pred_test[i]\n",
        "    })\n",
        "\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv('test_output_nonlm.csv', index=False)\n",
        "\n",
        "print(\"saved as test_output_nonlm.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDXw3PnBpym1"
      },
      "source": [
        "## LM-BASED APPROCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRHwABdhoRfO",
        "outputId": "d2aa15ba-2e02-4d3d-f7ab-1c0967ca831b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved to no_bias_test_output_distilbert.csv\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv(path_testset)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "label_names = [\"cultural agnostic\", \"cultural representative\", \"cultural exclusive\"]\n",
        "label_to_id = {label: i for i, label in enumerate(label_names)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"model_distilbert\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"model_distilbert\")\n",
        "\n",
        "def preprocess_test(row):\n",
        "    return f\"{row['name']} {row['description']} {row['category']} {row['subcategory']}\"\n",
        "\n",
        "test_df[\"text\"] = test_df.apply(preprocess_test, axis=1)\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    test_df[\"text\"].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_encodings)\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "\n",
        "test_df[\"label\"] = [id_to_label[i] for i in preds]\n",
        "\n",
        "test_df.drop(columns=[\"text\"], inplace=True)\n",
        "test_df.to_csv(\"test_output_distilbert.csv\", index=False)\n",
        "print(\"saved to test_output_distilbert.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m0pTJfnMsGf5",
        "jFBjKTtvprgL",
        "FDXw3PnBpym1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
